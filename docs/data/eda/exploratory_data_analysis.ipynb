{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploratory Data Analysis - Jupyter Notebook\n",
    "================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Loading the Data](#toc1_)    \n",
    "- [Data Overview](#toc2_)    \n",
    "- [Date Conversion](#toc3_)    \n",
    "- [Data Cleaning](#toc4_)    \n",
    "  - [Detect missing values](#toc4_1_)    \n",
    "  - [Redundancy](#toc4_2_)    \n",
    "    - [Drop Columns](#toc4_2_1_)    \n",
    "    - [Duplicates](#toc4_2_2_)    \n",
    "  - [Outlier detection and filtering](#toc4_3_)    \n",
    "- [Descriptive Statistics](#toc5_)    \n",
    "- [Data Analysis](#toc6_)    \n",
    "  - [Grouping Datasets](#toc6_1_)    \n",
    "  - [Correlation](#toc6_2_)    \n",
    "  - [Visualizing Time Series](#toc6_3_)    \n",
    "  - [Decomposition of variables](#toc6_4_)    \n",
    "  - [Distributions](#toc6_5_)    \n",
    "  - [Get the best-fitted distribution](#toc6_6_)    \n",
    "    - [n_sick](#toc6_6_1_)    \n",
    "    - [calls](#toc6_6_2_)    \n",
    "- [Data Quality](#toc7_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This jupyter notebook contains the results of the Exploratory Data Analysis (EDA).\n",
    "\n",
    "Ideas are partly taken from the books:  \n",
    "- Mukhiya, S. K., & Ahmed, U. (2020). Hands-On Exploratory Data Analysis with Python: Perform EDA Techniques to Understand, Summarize, and Investigate Your Data. Packt Publishing.\n",
    "- Atwan, T. A. (2022). Time Series Analysis with Python Cookbook: Practical recipes for exploratory data analysis, data preparation, forecasting, and model evaluation. Packt Publishing.\n",
    "\n",
    "\n",
    "# <a id='toc1_'></a>[Loading the Data](#toc0_)\n",
    "\n",
    "Loading the data into a pandas DataFrame because it is widely used in data science (Mukhiya & Ahmed, 2020, p. 28)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Operating system functionalities to join pathname components\n",
    "import os\n",
    "# Pandas for DataFrame and CSV handling\n",
    "import pandas as pd\n",
    "\n",
    "# Join the filepath of the raw data file\n",
    "filepath = os.path.join(\"..\", \"..\", \"..\", \"data\", \"raw\", \"sickness_table.csv\")\n",
    "print(\"### Load Data ### Filepath: \", filepath)\n",
    "\n",
    "# Read the data from CSV file\n",
    "data_raw = pd.read_csv(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[Data Overview](#toc0_)\n",
    "\n",
    "At first, get an overview about the data structure (Mukhiya & Ahmed, 2020, p. 30)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first 5 rows\n",
    "print(\"### Data Overview ### Structure (first 5 rows): \\n\", data_raw.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print some information about the DataFrame\n",
    "print(\"### Data Overview ### Data info: \\n\")\n",
    "data_raw.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is therefore multivariate time series data, with the following columns:\n",
    "- (unnamed): Row number\n",
    "- date: Date\n",
    "- n_sick: Number of emergency drivers who have registered a sick call\n",
    "- calls: Number of emergency calls\n",
    "- n_duty: Number of emergency drivers on duty\n",
    "- n_sby: Number of available substitute drivers\n",
    "- sby_need: Number of substitute drivers to be activated\n",
    "- dafted: Number of additional duty drivers that have to be activated if the number of on-call drivers are not sufficient\n",
    "\n",
    "The columns contain either integer or float values with exception of the date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc3_'></a>[Date Conversion](#toc0_)\n",
    "\n",
    "Sometimes it makes sense to convert some fields to simplify further processing. In the present data set, this is the case for the date.\n",
    "\n",
    "The date field is an object, so we need to convert it into a DateTime argument (Mukhiya & Ahmed, 2020, p. 76)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the data for conversion into a new variable\n",
    "data = data_raw.copy()\n",
    "# Convert the date objects into DateTime (raise an exception when parsing is invalid)\n",
    "data.date = data.date.apply(lambda x: pd.to_datetime(x, errors='raise', utc=True))\n",
    "print(\"### Date Conversion ### First 5 rows after convert date: \\n\", data.head(5))\n",
    "# Get the data types of the columns again\n",
    "print(\"### Date Conversion ### Data types after convert date: \\n\",data.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc4_'></a>[Data Cleaning](#toc0_)\n",
    "\n",
    "Data cleansing is useful to avoid errors and misinterpretation of data.\n",
    "\n",
    "## <a id='toc4_1_'></a>[Detect missing values](#toc0_)\n",
    "\n",
    "Identify NaN Values within the pandas dataframe using the isnull() function (Mukhiya & Ahmed, 2020, p. 113)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect missing values: In each columns\n",
    "print(\"### Data Cleaning ### Number of missing values in each column: \\n\", data.isnull().sum())\n",
    "# Detect missing values: Total number\n",
    "print(\"### Data Cleaning ### Total number of missing values: \", data.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no missing values within the dataset.\n",
    "\n",
    "## <a id='toc4_2_'></a>[Redundancy](#toc0_)\n",
    "\n",
    "### <a id='toc4_2_1_'></a>[Drop Columns](#toc0_)\n",
    "\n",
    "Irrelevant columns can be dropped (Mukhiya & Ahmed, 2020, p. 79)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discover columns that contain only a few different values\n",
    "print(\"### Data Cleaning ### Number of particular values in each column: \\n\", data.nunique())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first column only contains the row number, which is obsolete due to the row indices of the DataFrame and can be removed.  \n",
    "The column n_sby contains only one particular value, which can be removed in general. However, this is the value to be predicted, so the column should remain to keep the data structure compatible for the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the column with index 0 (unamed)\n",
    "data.drop(columns=data.columns[0], inplace=True)\n",
    "print(\"### Data Cleaning ### Raw data columns: \\n\",data_raw.columns)\n",
    "print(\"### Data Cleaning ### Cleaned columns: \\n\",data.columns)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column n_duty contains 3 different values. In what frequency do they occur?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the set of values witin the column and print the length\n",
    "n_duty_set = set(data.n_duty)\n",
    "print(\"### Data Cleaning ### Set of n_duty: \\n\",n_duty_set)\n",
    "n_duty_set_len = len(n_duty_set)\n",
    "print(\"### Data Cleaning ### Length of the set of n_duty: \\n\",n_duty_set_len)\n",
    "\n",
    "# Plot the distribution\n",
    "# Matplotlib for plots\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure().set_figwidth(15)\n",
    "# Create a figure\n",
    "fig, ax = plt.subplots()\n",
    "# Set figure width [inch]\n",
    "fig.set_figwidth(16)\n",
    "# Plot data\n",
    "ax.plot(data.date, data.n_duty, '.')\n",
    "# Set x-label\n",
    "ax.set_xlabel(\"date\")\n",
    "# Set y-label\n",
    "ax.set_ylabel(\"n_duty\")\n",
    "# Set title\n",
    "ax.set_title(\"Number of emergency drivers on duty\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The finding is, that the number of emergency drivers on duty was increased two times, the column can thus not be removed.\n",
    "\n",
    "The column dafted contains the number of additional duty drivers that have to be activated if the number of on-call drivers are not sufficient. It can be assumed that this is the difference between sby_need and n_sby if it is positive. Let's check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the rows if the value of the column dafted ist not zero:\n",
    "data_dafted_rows = data.loc[(data['dafted'] != 0)]\n",
    "print(\"### Data Cleaning ### Data rows with values dafted != 0: \\n\", data_dafted_rows)\n",
    "# Counter for rows for which the assumption is false (the difference between sby_need and n_sby is equal to dafted in case dafted is not zero)\n",
    "couter_dafted_is_required = 0\n",
    "# Counter to check the number of loop passes\n",
    "counter_loop_passes = 0\n",
    "for index, row in data_dafted_rows.iterrows():\n",
    "    counter_loop_passes += 1\n",
    "    if ((row.sby_need - row.n_sby) != row.dafted):\n",
    "        print(\"### Data Cleaning ### Check dafted: #### FALSE ####\\n\")\n",
    "        # Increment the counter\n",
    "        couter_dafted_is_required += 1\n",
    "print(\"### Data Cleaning ### Check loop passes: \", counter_loop_passes)\n",
    "print(\"### Data Cleaning ### Number of loops which have shown that the dafted column is required: \", couter_dafted_is_required)  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the column dafted could be dropped because it has a strong correlation with sby_need. But we leave the column in for now and analyze the correlation to confirm this finding on a later stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the column dafted\n",
    "#data.drop(columns='dafted', inplace=True)\n",
    "print(\"### Data Cleaning ### Raw data columns: \\n\",data_raw.columns)\n",
    "print(\"### Data Cleaning ### Cleaned columns: \\n\",data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_2_2_'></a>[Duplicates](#toc0_)\n",
    "\n",
    "Duplicate rows are also redundancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get duplicated rows\n",
    "print(\"### Data Cleaning ### Check duplicate rows and count the number: \\n\",data_raw.duplicated().sum())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no duplicate rows in the dataset.\n",
    "\n",
    "## <a id='toc4_3_'></a>[Outlier detection and filtering](#toc0_)\n",
    "\n",
    "(Mukhiya & Ahmed, 2020, p. 126) and (Atwan 2022, p. 245)\n",
    "\n",
    "See the chapter about the distributions and box-plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc5_'></a>[Descriptive Statistics](#toc0_)\n",
    "\n",
    "Checking the data after cleansing unsing descriptive statistics (Mukhiya & Ahmed, 2020, p. 76)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print some information about the DataFrame\n",
    "print(\"### Descriptive Statistics ### DataFrame info: \\n\")\n",
    "data.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze the data using descriptive statistics (Mukhiya & Ahmed, 2020, p. 145-158):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"### Descriptive Statistics ### DataFrame describe: \\n\", data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Findings:\n",
    "- every column contains 1152 elements, the data set therefore has no gaps\n",
    "- n_sby has a standard deviation of zero, is thus constant\n",
    "- n_sick varies between 36 and 119 \n",
    "- the number of calls have a high standard deviation, i.e. fluctuate strongly\n",
    "- the number of stubstitude drivers to be activated is on average approx. 35\n",
    "\n",
    "# <a id='toc6_'></a>[Data Analysis](#toc0_)\n",
    "\n",
    "This is the most important part to get insights from the data (Mukhiya & Ahmed, 2020, p. 81).\n",
    "\n",
    "## <a id='toc6_1_'></a>[Grouping Datasets](#toc0_)\n",
    "\n",
    "(Mukhiya & Ahmed, 2020, p. 163).\n",
    "\n",
    "It might be useful to group the data set based on seasonality, for example. This is to be analyzed in more detail during the data preparation.\n",
    "\n",
    "## <a id='toc6_2_'></a>[Correlation](#toc0_)\n",
    "\n",
    "Finding correlated columns helps to identify the relation of the potential features (Mukhiya & Ahmed, 2020, p. 285)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using seaborn for statistical data visualization\n",
    "import seaborn as sns\n",
    "\n",
    "# Finding correlated colums with heatmap\n",
    "sns.heatmap(data.corr(), annot=True, fmt='.2f', linewidths=2)\n",
    "# and pairplot()\n",
    "sns.pairplot(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Findings:\n",
    "- n_sick has a positive correlation with n_duty\n",
    "- calls has a weak positive correlation with n_duty\n",
    "- calls has a strong positive correlation with sby_need\n",
    "- sby_need has a very strong positive correlation with dafted\n",
    "\n",
    "## <a id='toc6_3_'></a>[Visualizing Time Series](#toc0_)\n",
    "\n",
    "Visualization enables further insights (Mukhiya & Ahmed, 2020, p. 224)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with 6 subplots\n",
    "fig, (ax1, ax2, ax3, ax4, ax5, ax6) = plt.subplots(6,1)\n",
    "# Set figure width and height[inch]\n",
    "fig.set_figwidth(16)\n",
    "fig.set_figheight(30)\n",
    "\n",
    "# Plot data: n_sick\n",
    "ax1.plot(data.date, data.n_sick, '.')\n",
    "# Set x-label\n",
    "ax1.set_xlabel(\"date\")\n",
    "# Set y-label\n",
    "ax1.set_ylabel(\"n_sick\")\n",
    "# Set title\n",
    "ax1.set_title(\"Number of sick emergency drivers\")\n",
    "\n",
    "# Plot data: calls\n",
    "ax2.plot(data.date, data.calls, '.')\n",
    "# Set x-label\n",
    "ax2.set_xlabel(\"date\")\n",
    "# Set y-label\n",
    "ax2.set_ylabel(\"calls\")\n",
    "# Set title\n",
    "ax2.set_title(\"Number of emergency calls\")\n",
    "\n",
    "# Plot data: n_duty\n",
    "ax3.plot(data.date, data.n_duty, '.')\n",
    "# Set x-label\n",
    "ax3.set_xlabel(\"date\")\n",
    "# Set y-label\n",
    "ax3.set_ylabel(\"n_duty\")\n",
    "# Set title\n",
    "ax3.set_title(\"Number of emergency drivers on duty\")\n",
    "\n",
    "# Plot data: n_sby\n",
    "ax4.plot(data.date, data.n_sby, '.')\n",
    "# Set x-label\n",
    "ax4.set_xlabel(\"date\")\n",
    "# Set y-label\n",
    "ax4.set_ylabel(\"n_sby\")\n",
    "# Set title\n",
    "ax4.set_title(\"Number of available substitude drivers\")\n",
    "\n",
    "# Plot data: sby_need\n",
    "ax5.plot(data.date, data.sby_need, '.')\n",
    "# Set x-label\n",
    "ax5.set_xlabel(\"date\")\n",
    "# Set y-label\n",
    "ax5.set_ylabel(\"sby_need\")\n",
    "# Set title\n",
    "ax5.set_title(\"Number of substitude drivers to be activated\")\n",
    "\n",
    "# Plot data: dafted\n",
    "ax6.plot(data.date, data.dafted, '.')\n",
    "# Set x-label\n",
    "ax6.set_xlabel(\"date\")\n",
    "# Set y-label\n",
    "ax6.set_ylabel(\"dafted\")\n",
    "# Set title\n",
    "ax6.set_title(\"Number of additional duty drivers\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Findings:\n",
    "- In principle, a seasonal pattern and a slight upward trend can be discerned in the data\n",
    "- The finding that the two columns sby_need and dafted are strongly correlated can be confirmed visually\n",
    "\n",
    "## <a id='toc6_4_'></a>[Decomposition of variables](#toc0_)\n",
    "\n",
    "A decomposition of variables for example the date due to seasonality could be useful and should be analyzed further during data preparation.\n",
    "\n",
    "## <a id='toc6_5_'></a>[Distributions](#toc0_)\n",
    "\n",
    "Visual representation of the distributions can also help in analyzing the data (Atwan 2022, p. 258)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with 6 subplots\n",
    "#fig, (ax1, ax2, ax3, ax4, ax5, ax6) = plt.subplots(6,1)\n",
    "# Set figure width and height[inch]\n",
    "#fig.set_figwidth(16)\n",
    "#fig.set_figheight(30)\n",
    "\n",
    "# Create a figure with 6 subplots\n",
    "fig, axes = plt.subplots(6, 1, figsize=(18, 30))\n",
    "fig.suptitle('Histograms')\n",
    "\n",
    "# n_sick\n",
    "sns.histplot(ax=axes[0], data=data.n_sick)\n",
    "# Set title\n",
    "axes[0].set_title(\"Distribution of the number of sick emergency drivers\")\n",
    "\n",
    "# calls\n",
    "sns.histplot(ax=axes[1], data=data.calls)\n",
    "# Set title\n",
    "axes[1].set_title(\"Distribution of the number of emergency calls\")\n",
    "\n",
    "# n_duty\n",
    "sns.histplot(ax=axes[2], data=data.n_duty)\n",
    "# Set title\n",
    "axes[2].set_title(\"Distribution of the number of emergency drivers on duty\")\n",
    "\n",
    "# n_sby\n",
    "sns.histplot(ax=axes[3], data=data.n_sby)\n",
    "# Set title\n",
    "axes[3].set_title(\"Distribution of the number of available substitude drivers\")\n",
    "\n",
    "# sby_need\n",
    "sns.histplot(ax=axes[4], data=data.sby_need)\n",
    "# Set title\n",
    "axes[4].set_title(\"Distribution of the number of substitude drivers to be activated\")\n",
    "\n",
    "# dafted\n",
    "sns.histplot(ax=axes[5], data=data.dafted)\n",
    "# Set title\n",
    "axes[5].set_title(\"Distribution of the number of additional duty drivers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the frequency distributions, a first analysis can be made regarding outliers:\n",
    "- The distributions for n_sick and calls look inconspicuous, outliers cannot be assumed according to the present state of knowledge, since the range of values looks plausible\n",
    "- The distributions to sby_need and dafted may rather suggest outliers here, which should be investigated in more detail during data preparation\n",
    "\n",
    "## <a id='toc6_6_'></a>[Get the best-fitted distribution](#toc0_)\n",
    "\n",
    "Identify the best-fitted distribution for specific data colums.\n",
    "\n",
    "### <a id='toc6_6_1_'></a>[n_sick](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library for fitting data to distributions\n",
    "from fitter import Fitter\n",
    "\n",
    "# Create a fitter instance\n",
    "f = Fitter(data.n_sick,\n",
    "#            distributions=['norm',\n",
    "#                          'lognorm',\n",
    "#                          \"beta\",\n",
    "#                          \"burr\",\n",
    "#                          \"norm\"]\n",
    ")\n",
    "\n",
    "# # Comment out because it is compute-intensive: Activate if new results are required\n",
    "# Fit the data\n",
    "#f.fit()\n",
    "# Generate the fitted distribution summary\n",
    "#f.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the sumsquare_error value the best distribution is the johnsonsb distribution (a Johnson SB continuous random variable).\n",
    "\n",
    "### <a id='toc6_6_2_'></a>[calls](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a fitter instance\n",
    "f = Fitter(data.calls,\n",
    "#            distributions=['norm',\n",
    "#                          'lognorm',\n",
    "#                          \"beta\",\n",
    "#                          \"burr\",\n",
    "#                          \"norm\"]\n",
    ")\n",
    "           \n",
    "# # Comment out because it is compute-intensive: Activate if new results are required\n",
    "# Fit the data\n",
    "#f.fit()\n",
    "# Generate the fitted distribution summary\n",
    "#f.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the sumsquare_error value the best distribution is the gennorm distribution (a generalized normal continuous random variable).\n",
    "\n",
    "## Boxplot\n",
    "\n",
    "Boxplots are a good choice to analyze potential outliers (Atwan 2022, p. 259)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with 6 subplots\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "fig.suptitle('Boxplots')\n",
    "\n",
    "# n_sick\n",
    "sns.boxplot(ax=axes[0, 0], data=data.n_sick)\n",
    "# Set y-label\n",
    "axes[0, 0].set_ylabel(\"n_sick\")\n",
    "# Set title\n",
    "axes[0, 0].set_title(\"Number of sick emergency drivers\")\n",
    "\n",
    "# calls\n",
    "sns.boxplot(ax=axes[0, 1], data=data.calls)\n",
    "# Set y-label\n",
    "axes[0, 1].set_ylabel(\"calls\")\n",
    "# Set title\n",
    "axes[0, 1].set_title(\"Number of emergency calls\")\n",
    "\n",
    "# n_duty\n",
    "sns.boxplot(ax=axes[0, 2], data=data.n_duty)\n",
    "# Set y-label\n",
    "axes[0, 2].set_ylabel(\"n_duty\")\n",
    "# Set title\n",
    "axes[0, 2].set_title(\"Number of emergency drivers on duty\")\n",
    "\n",
    "# n_sby\n",
    "sns.boxplot(ax=axes[1, 0], data=data.n_sby)\n",
    "# Set y-label\n",
    "axes[1, 0].set_ylabel(\"n_sby\")\n",
    "# Set title\n",
    "axes[1, 0].set_title(\"Number of available substitude drivers\")\n",
    "\n",
    "# sby_need\n",
    "sns.boxplot(ax=axes[1, 1], data=data.sby_need)\n",
    "# Set y-label\n",
    "axes[1, 1].set_ylabel(\"sby_need\")\n",
    "# Set title\n",
    "axes[1, 1].set_title(\"Number of substitude drivers to be activated\")\n",
    "\n",
    "# dafted\n",
    "sns.boxplot(ax=axes[1, 2], data=data.dafted)\n",
    "# Set y-label\n",
    "axes[1, 2].set_ylabel(\"dafted\")\n",
    "# Set title\n",
    "axes[1, 2].set_title(\"Number of additional duty drivers\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the boxplots, the following potential outliers can be suspected:\n",
    "- n_sick and calls values outside the whiskers, which could be defined as outliers, occur rather rarely\n",
    "- n_duty and n_sby do not contain outliers\n",
    "- expected several at sby_need and dafted\n",
    "\n",
    "Overall, without deeper domain knowledge, it is hard to make a firm statement as to whether these are really outliers. The alleged outliers in n_sick and calls are within a realistic range, but can be handled as outliers. In addition, the different value ranges must be taken into account. For sb_need and dafted, statistical evaluations are not very meaningful with regard to outliers, since they are generally already \"outliers\"?\n",
    "\n",
    "# <a id='toc7_'></a>[Data Quality](#toc0_)\n",
    "\n",
    "Basically, it is multivariate time series data with good quality. Only the date column was converted for better processing and no missing values could be detected. In some cases, variables show strong correlations with each other what needs to be considered in the data preparation phase and the feature selection phase.\n",
    "The data types of the variables match the specification and the value ranges are mostly plausible, but should be checked for outliers in individual cases.\n",
    "In principle, a seasonal pattern and a slight upward trend can be discerned in the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dutySchedule_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6913ccc9ce5f215f64980cc9576bfffc64b3ea55643bf3a7938d14e6344a8d8b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
