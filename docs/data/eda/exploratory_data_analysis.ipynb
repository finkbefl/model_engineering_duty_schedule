{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploratory Data Analysis - Jupyter Notebook\n",
    "================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Loading the Data](#toc1_)    \n",
    "- [Data Overview](#toc2_)    \n",
    "- [Date Conversion](#toc3_)    \n",
    "- [Data Cleaning](#toc4_)    \n",
    "  - [Detect missing values](#toc4_1_)    \n",
    "  - [Redundancy](#toc4_2_)    \n",
    "    - [Drop Columns](#toc4_2_1_)    \n",
    "    - [Duplicates](#toc4_2_2_)    \n",
    "- [Descriptive Statistics](#toc5_)    \n",
    "- [Data Analysis](#toc6_)    \n",
    "  - [Correlation](#toc6_1_)    \n",
    "  - [Visualizing Time Series](#toc6_2_)    \n",
    "  - [Decomposition of variables](#toc6_3_)    \n",
    "  - [Distributions](#toc6_4_)    \n",
    "  - [Get the best-fitted distribution](#toc6_5_)    \n",
    "    - [n_sick](#toc6_5_1_)    \n",
    "    - [calls](#toc6_5_2_)    \n",
    "  - [Boxplot](#toc6_6_)    \n",
    "  - [Skewness](#toc6_7_)    \n",
    "  - [Stationarity](#toc6_8_)    \n",
    "- [Data Quality](#toc7_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This jupyter notebook contains the results of the Exploratory Data Analysis (EDA).\n",
    "\n",
    "Ideas are partly taken from the books:  \n",
    "- Mukhiya, S. K., & Ahmed, U. (2020). Hands-On Exploratory Data Analysis with Python: Perform EDA Techniques to Understand, Summarize, and Investigate Your Data. Packt Publishing.\n",
    "- Atwan, T. A. (2022). Time Series Analysis with Python Cookbook: Practical recipes for exploratory data analysis, data preparation, forecasting, and model evaluation. Packt Publishing.\n",
    "\n",
    "\n",
    "# <a id='toc1_'></a>[Loading the Data](#toc0_)\n",
    "\n",
    "Loading the data into a pandas DataFrame because it is widely used in data science (Mukhiya & Ahmed, 2020, p. 28)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Operating system functionalities to join pathname components\n",
    "import os\n",
    "# Pandas for DataFrame and CSV handling\n",
    "import pandas as pd\n",
    "\n",
    "# Join the filepath of the raw data file\n",
    "filepath = os.path.join(\"..\", \"..\", \"..\", \"data\", \"raw\", \"sickness_table.csv\")\n",
    "print(\"### Load Data ### Filepath: \", filepath)\n",
    "\n",
    "# Read the data from CSV file\n",
    "data_raw = pd.read_csv(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[Data Overview](#toc0_)\n",
    "\n",
    "At first, get an overview about the data structure (Mukhiya & Ahmed, 2020, p. 30)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first 5 rows\n",
    "print(\"### Data Overview ### Structure (first 5 rows): \\n\", data_raw.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print some information about the DataFrame\n",
    "print(\"### Data Overview ### Data info: \\n\")\n",
    "data_raw.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is therefore multivariate time series data, with the following columns:\n",
    "- (unnamed): Row number\n",
    "- date: Date\n",
    "- n_sick: Number of emergency drivers who have registered a sick call\n",
    "- calls: Number of emergency calls\n",
    "- n_duty: Number of emergency drivers on duty\n",
    "- n_sby: Number of available substitute drivers\n",
    "- sby_need: Number of substitute drivers to be activated\n",
    "- dafted: Number of additional duty drivers that have to be activated if the number of on-call drivers are not sufficient\n",
    "\n",
    "The columns contain either integer or float values with exception of the date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc3_'></a>[Date Conversion](#toc0_)\n",
    "\n",
    "Sometimes it makes sense to convert some fields to simplify further processing. In the present data set, this is the case for the date.\n",
    "\n",
    "The date field is an object, so we need to convert it into a DateTime argument (Mukhiya & Ahmed, 2020, p. 76)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the data for conversion into a new variable\n",
    "data = data_raw.copy()\n",
    "# Convert the date objects into DateTime (raise an exception when parsing is invalid)\n",
    "data.date = data.date.apply(lambda x: pd.to_datetime(x, errors='raise', utc=True))\n",
    "print(\"### Date Conversion ### First 5 rows after convert date: \\n\", data.head(5))\n",
    "# Get the data types of the columns again\n",
    "print(\"### Date Conversion ### Data types after convert date: \\n\",data.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc4_'></a>[Data Cleaning](#toc0_)\n",
    "\n",
    "Data cleansing is useful to avoid errors and misinterpretation of data.\n",
    "\n",
    "## <a id='toc4_1_'></a>[Detect missing values](#toc0_)\n",
    "\n",
    "Identify NaN Values within the pandas dataframe using the isnull() function (Mukhiya & Ahmed, 2020, p. 113)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect missing values: In each columns\n",
    "print(\"### Data Cleaning ### Number of missing values in each column: \\n\", data.isnull().sum())\n",
    "# Detect missing values: Total number\n",
    "print(\"### Data Cleaning ### Total number of missing values: \", data.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no missing values within the dataset.\n",
    "\n",
    "## <a id='toc4_2_'></a>[Redundancy](#toc0_)\n",
    "\n",
    "### <a id='toc4_2_1_'></a>[Drop Columns](#toc0_)\n",
    "\n",
    "Irrelevant columns can be dropped (Mukhiya & Ahmed, 2020, p. 79)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discover columns that contain only a few different values\n",
    "print(\"### Data Cleaning ### Number of particular values in each column: \\n\", data.nunique())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first column only contains the row number, which is obsolete due to the row indices of the DataFrame and can be removed.  \n",
    "The column n_sby contains only one particular value, which can be removed in general. However, this is the value to be predicted, so the column should remain to keep the data structure compatible for the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the column with index 0 (unamed)\n",
    "data.drop(columns=data.columns[0], inplace=True)\n",
    "print(\"### Data Cleaning ### Raw data columns: \\n\",data_raw.columns)\n",
    "print(\"### Data Cleaning ### Cleaned columns: \\n\",data.columns)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column n_duty contains 3 different values. In what frequency do they occur?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the set of values witin the column and print the length\n",
    "n_duty_set = set(data.n_duty)\n",
    "print(\"### Data Cleaning ### Set of n_duty: \\n\",n_duty_set)\n",
    "n_duty_set_len = len(n_duty_set)\n",
    "print(\"### Data Cleaning ### Length of the set of n_duty: \\n\",n_duty_set_len)\n",
    "\n",
    "# Plot the distribution\n",
    "# Matplotlib for plots\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure().set_figwidth(15)\n",
    "# Create a figure\n",
    "fig, ax = plt.subplots()\n",
    "# Set figure width [inch]\n",
    "fig.set_figwidth(16)\n",
    "# Plot data\n",
    "ax.plot(data.date, data.n_duty, '.')\n",
    "# Set x-label\n",
    "ax.set_xlabel(\"date\")\n",
    "# Set y-label\n",
    "ax.set_ylabel(\"n_duty\")\n",
    "# Set title\n",
    "ax.set_title(\"Number of emergency drivers on duty\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The finding is, that the number of emergency drivers on duty was increased two times, the column can thus not be removed.\n",
    "\n",
    "The column dafted contains the number of additional duty drivers that have to be activated if the number of on-call drivers are not sufficient. It can be assumed that this is the difference between sby_need and n_sby if it is positive. Let's check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the rows if the value of the column dafted ist not zero:\n",
    "data_dafted_rows = data.loc[(data['dafted'] != 0)]\n",
    "print(\"### Data Cleaning ### Data rows with values dafted != 0: \\n\", data_dafted_rows)\n",
    "# Counter for rows for which the assumption is false (the difference between sby_need and n_sby is equal to dafted in case dafted is not zero)\n",
    "couter_dafted_is_required = 0\n",
    "# Counter to check the number of loop passes\n",
    "counter_loop_passes = 0\n",
    "for index, row in data_dafted_rows.iterrows():\n",
    "    counter_loop_passes += 1\n",
    "    if ((row.sby_need - row.n_sby) != row.dafted):\n",
    "        print(\"### Data Cleaning ### Check dafted: #### FALSE ####\\n\")\n",
    "        # Increment the counter\n",
    "        couter_dafted_is_required += 1\n",
    "print(\"### Data Cleaning ### Check loop passes: \", counter_loop_passes)\n",
    "print(\"### Data Cleaning ### Number of loops which have shown that the dafted column is required: \", couter_dafted_is_required)  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the column dafted could be dropped because it has a strong correlation with sby_need. But we leave the column in for now and analyze the correlation to confirm this finding on a later stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the column dafted\n",
    "#data.drop(columns='dafted', inplace=True)\n",
    "print(\"### Data Cleaning ### Raw data columns: \\n\",data_raw.columns)\n",
    "print(\"### Data Cleaning ### Cleaned columns: \\n\",data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_2_2_'></a>[Duplicates](#toc0_)\n",
    "\n",
    "Duplicate rows are also redundancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get duplicated rows\n",
    "print(\"### Data Cleaning ### Check duplicate rows and count the number: \\n\",data_raw.duplicated().sum())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no duplicate rows in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc5_'></a>[Descriptive Statistics](#toc0_)\n",
    "\n",
    "Checking the data after cleansing unsing descriptive statistics (Mukhiya & Ahmed, 2020, p. 76)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print some information about the DataFrame\n",
    "print(\"### Descriptive Statistics ### DataFrame info: \\n\")\n",
    "data.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze the data using descriptive statistics (Mukhiya & Ahmed, 2020, p. 145-158):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"### Descriptive Statistics ### DataFrame describe: \\n\", data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Findings:\n",
    "- every column contains 1152 elements, the data set therefore has no gaps\n",
    "- n_sby has a standard deviation of zero, is thus constant\n",
    "- n_sick varies between 36 and 119 \n",
    "- the number of calls have a high standard deviation, i.e. fluctuate strongly\n",
    "- the number of stubstitude drivers to be activated is on average approx. 35\n",
    "\n",
    "# <a id='toc6_'></a>[Data Analysis](#toc0_)\n",
    "\n",
    "This is the most important part to get insights from the data (Mukhiya & Ahmed, 2020, p. 81).\n",
    "\n",
    "## <a id='toc6_1_'></a>[Correlation](#toc0_)\n",
    "\n",
    "Finding correlated columns helps to identify the relation of the potential features (Mukhiya & Ahmed, 2020, p. 285)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using seaborn for statistical data visualization\n",
    "import seaborn as sns\n",
    "\n",
    "# Finding correlated colums with heatmap\n",
    "sns.heatmap(data.corr(), annot=True, fmt='.2f', linewidths=2)\n",
    "# and pairplot()\n",
    "sns.pairplot(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Findings:\n",
    "- n_sick has a positive correlation with n_duty\n",
    "- calls has a weak positive correlation with n_duty\n",
    "- calls has a strong positive correlation with sby_need\n",
    "- sby_need has a very strong positive correlation with dafted\n",
    "\n",
    "It should be noted, that dropping the column drafted is the better choice than dropping sby_need, because sby_need contains more information! This can be checked by calculating it the other way around. Adding the value of n_sby to every value in dafted which is not zero is not totally equal to the column dafted, exactly at the points where dafted has values below the value of n_sby.\n",
    "\n",
    "## <a id='toc6_2_'></a>[Visualizing Time Series](#toc0_)\n",
    "\n",
    "Visualization enables further insights (Mukhiya & Ahmed, 2020, p. 224)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with 6 subplots\n",
    "fig, (ax1, ax2, ax3, ax4, ax5, ax6) = plt.subplots(6,1)\n",
    "# Set figure width and height[inch]\n",
    "fig.set_figwidth(16)\n",
    "fig.set_figheight(30)\n",
    "\n",
    "# Plot data: n_sick\n",
    "ax1.plot(data.date, data.n_sick, '.')\n",
    "# Set x-label\n",
    "ax1.set_xlabel(\"date\")\n",
    "# Set y-label\n",
    "ax1.set_ylabel(\"n_sick\")\n",
    "# Set title\n",
    "ax1.set_title(\"Number of sick emergency drivers\")\n",
    "\n",
    "# Plot data: calls\n",
    "ax2.plot(data.date, data.calls, '.')\n",
    "# Set x-label\n",
    "ax2.set_xlabel(\"date\")\n",
    "# Set y-label\n",
    "ax2.set_ylabel(\"calls\")\n",
    "# Set title\n",
    "ax2.set_title(\"Number of emergency calls\")\n",
    "\n",
    "# Plot data: n_duty\n",
    "ax3.plot(data.date, data.n_duty, '.')\n",
    "# Set x-label\n",
    "ax3.set_xlabel(\"date\")\n",
    "# Set y-label\n",
    "ax3.set_ylabel(\"n_duty\")\n",
    "# Set title\n",
    "ax3.set_title(\"Number of emergency drivers on duty\")\n",
    "\n",
    "# Plot data: n_sby\n",
    "ax4.plot(data.date, data.n_sby, '.')\n",
    "# Set x-label\n",
    "ax4.set_xlabel(\"date\")\n",
    "# Set y-label\n",
    "ax4.set_ylabel(\"n_sby\")\n",
    "# Set title\n",
    "ax4.set_title(\"Number of available substitude drivers\")\n",
    "\n",
    "# Plot data: sby_need\n",
    "ax5.plot(data.date, data.sby_need, '.')\n",
    "# Set x-label\n",
    "ax5.set_xlabel(\"date\")\n",
    "# Set y-label\n",
    "ax5.set_ylabel(\"sby_need\")\n",
    "# Set title\n",
    "ax5.set_title(\"Number of substitude drivers to be activated\")\n",
    "\n",
    "# Plot data: dafted\n",
    "ax6.plot(data.date, data.dafted, '.')\n",
    "# Set x-label\n",
    "ax6.set_xlabel(\"date\")\n",
    "# Set y-label\n",
    "ax6.set_ylabel(\"dafted\")\n",
    "# Set title\n",
    "ax6.set_title(\"Number of additional duty drivers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Findings:\n",
    "- In principle, a seasonal pattern and a slight upward trend can be discerned in the data\n",
    "- The finding that the two columns sby_need and dafted are strongly correlated can be confirmed visually\n",
    "\n",
    "## <a id='toc6_3_'></a>[Decomposition of variables](#toc0_)\n",
    "\n",
    "A decomposition of variables into the major components will help during the modeling process (Atwan 2022, p. 299)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using statsmodel for decomposition analysis\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "# Set datetime as index so that the frequency of the data can be automatically detected\n",
    "data = data.set_index(data.date)\n",
    "\n",
    "# Decompose the data columns\n",
    "# n_sick\n",
    "# Create a figure with 4 subplots\n",
    "fig, axes = plt.subplots(4,1)\n",
    "# Set figure width and height[inch]\n",
    "fig.set_figwidth(16)\n",
    "fig.set_figheight(20)\n",
    "plt.suptitle(\"n_sick\")\n",
    "res = seasonal_decompose(data.n_sick, model='additive', period=365) \n",
    "res.observed.plot(ax=axes[0])\n",
    "axes[0].set_ylabel(\"observed\")\n",
    "res.trend.plot(ax=axes[1])\n",
    "axes[1].set_ylabel(\"trend\")\n",
    "res.seasonal.plot(ax=axes[2])\n",
    "axes[2].set_ylabel(\"seasonal\")\n",
    "res.resid.plot(ax=axes[3])\n",
    "axes[3].set_ylabel(\"resid\")\n",
    "plt.show()\n",
    "# calls\n",
    "# Create a figure with 4 subplots\n",
    "fig, axes = plt.subplots(4,1)\n",
    "# Set figure width and height[inch]\n",
    "fig.set_figwidth(16)\n",
    "fig.set_figheight(20)\n",
    "plt.suptitle(\"calls\")\n",
    "res = seasonal_decompose(data.calls, model='additive', period=365) \n",
    "res.observed.plot(ax=axes[0])\n",
    "axes[0].set_ylabel(\"observed\")\n",
    "res.trend.plot(ax=axes[1])\n",
    "axes[1].set_ylabel(\"trend\")\n",
    "res.seasonal.plot(ax=axes[2])\n",
    "axes[2].set_ylabel(\"seasonal\")\n",
    "res.resid.plot(ax=axes[3])\n",
    "axes[3].set_ylabel(\"resid\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an upward trend for both calls and n_sick, and a certain periodicity can be inferred from the decomposition as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc6_4_'></a>[Distributions](#toc0_)\n",
    "\n",
    "Visual representation of the distributions can also help in analyzing the data (Atwan 2022, p. 258)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with 6 subplots\n",
    "#fig, (ax1, ax2, ax3, ax4, ax5, ax6) = plt.subplots(6,1)\n",
    "# Set figure width and height[inch]\n",
    "#fig.set_figwidth(16)\n",
    "#fig.set_figheight(30)\n",
    "\n",
    "# Create a figure with 6 subplots\n",
    "fig, axes = plt.subplots(6, 1, figsize=(18, 30))\n",
    "fig.suptitle('Histograms')\n",
    "\n",
    "# n_sick\n",
    "sns.histplot(ax=axes[0], data=data.n_sick)\n",
    "# Set title\n",
    "axes[0].set_title(\"Distribution of the number of sick emergency drivers\")\n",
    "\n",
    "# calls\n",
    "sns.histplot(ax=axes[1], data=data.calls)\n",
    "# Set title\n",
    "axes[1].set_title(\"Distribution of the number of emergency calls\")\n",
    "\n",
    "# n_duty\n",
    "sns.histplot(ax=axes[2], data=data.n_duty)\n",
    "# Set title\n",
    "axes[2].set_title(\"Distribution of the number of emergency drivers on duty\")\n",
    "\n",
    "# n_sby\n",
    "sns.histplot(ax=axes[3], data=data.n_sby)\n",
    "# Set title\n",
    "axes[3].set_title(\"Distribution of the number of available substitude drivers\")\n",
    "\n",
    "# sby_need\n",
    "sns.histplot(ax=axes[4], data=data.sby_need)\n",
    "# Set title\n",
    "axes[4].set_title(\"Distribution of the number of substitude drivers to be activated\")\n",
    "\n",
    "# dafted\n",
    "sns.histplot(ax=axes[5], data=data.dafted)\n",
    "# Set title\n",
    "axes[5].set_title(\"Distribution of the number of additional duty drivers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the frequency distributions, a first analysis can be made regarding outliers:\n",
    "- The distributions for n_sick and calls look inconspicuous, outliers cannot be assumed according to the present state of knowledge, since the range of values looks plausible\n",
    "- The distributions to sby_need and dafted are extremly positive skewed. A statement about possible outliers is therefore not possible here.\n",
    "\n",
    "## <a id='toc6_5_'></a>[Get the best-fitted distribution](#toc0_)\n",
    "\n",
    "Identify the best-fitted distribution for specific data colums.\n",
    "\n",
    "### <a id='toc6_5_1_'></a>[n_sick](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library for fitting data to distributions\n",
    "from fitter import Fitter\n",
    "\n",
    "# Create a fitter instance\n",
    "f = Fitter(data.n_sick,\n",
    "#            distributions=['norm',\n",
    "#                          'lognorm',\n",
    "#                          \"beta\",\n",
    "#                          \"burr\",\n",
    "#                          \"norm\"]\n",
    ")\n",
    "\n",
    "# # Comment out because it is compute-intensive: Activate if new results are required\n",
    "# Fit the data\n",
    "#f.fit()\n",
    "# Generate the fitted distribution summary\n",
    "#f.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the sumsquare_error value the best distribution is the johnsonsb distribution (a Johnson SB continuous random variable).\n",
    "\n",
    "### <a id='toc6_5_2_'></a>[calls](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a fitter instance\n",
    "f = Fitter(data.calls,\n",
    "#            distributions=['norm',\n",
    "#                          'lognorm',\n",
    "#                          \"beta\",\n",
    "#                          \"burr\",\n",
    "#                          \"norm\"]\n",
    ")\n",
    "           \n",
    "# # Comment out because it is compute-intensive: Activate if new results are required\n",
    "# Fit the data\n",
    "#f.fit()\n",
    "# Generate the fitted distribution summary\n",
    "#f.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the sumsquare_error value the best distribution is the gennorm distribution (a generalized normal continuous random variable).\n",
    "\n",
    "## <a id='toc6_6_'></a>[Boxplot](#toc0_)\n",
    "\n",
    "Boxplots are a good choice to analyze potential outliers (Atwan 2022, p. 259)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with 6 subplots\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "fig.suptitle('Boxplots')\n",
    "\n",
    "# n_sick\n",
    "sns.boxplot(ax=axes[0, 0], data=data.n_sick)\n",
    "# Set y-label\n",
    "axes[0, 0].set_ylabel(\"n_sick\")\n",
    "# Set title\n",
    "axes[0, 0].set_title(\"Number of sick emergency drivers\")\n",
    "\n",
    "# calls\n",
    "sns.boxplot(ax=axes[0, 1], data=data.calls)\n",
    "# Set y-label\n",
    "axes[0, 1].set_ylabel(\"calls\")\n",
    "# Set title\n",
    "axes[0, 1].set_title(\"Number of emergency calls\")\n",
    "\n",
    "# n_duty\n",
    "sns.boxplot(ax=axes[0, 2], data=data.n_duty)\n",
    "# Set y-label\n",
    "axes[0, 2].set_ylabel(\"n_duty\")\n",
    "# Set title\n",
    "axes[0, 2].set_title(\"Number of emergency drivers on duty\")\n",
    "\n",
    "# n_sby\n",
    "sns.boxplot(ax=axes[1, 0], data=data.n_sby)\n",
    "# Set y-label\n",
    "axes[1, 0].set_ylabel(\"n_sby\")\n",
    "# Set title\n",
    "axes[1, 0].set_title(\"Number of available substitude drivers\")\n",
    "\n",
    "# sby_need\n",
    "sns.boxplot(ax=axes[1, 1], data=data.sby_need)\n",
    "# Set y-label\n",
    "axes[1, 1].set_ylabel(\"sby_need\")\n",
    "# Set title\n",
    "axes[1, 1].set_title(\"Number of substitude drivers to be activated\")\n",
    "\n",
    "# dafted\n",
    "sns.boxplot(ax=axes[1, 2], data=data.dafted)\n",
    "# Set y-label\n",
    "axes[1, 2].set_ylabel(\"dafted\")\n",
    "# Set title\n",
    "axes[1, 2].set_title(\"Number of additional duty drivers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the boxplots, the following potential outliers can be suspected:\n",
    "- Only a few n_sick and calls values outside the whiskers, which could be defined as outliers\n",
    "- n_duty and n_sby do not contain outliers\n",
    "- Boxplots also confirm the positive skew in sby_need and dafted.\n",
    "\n",
    "## <a id='toc6_7_'></a>[Skewness](#toc0_)\n",
    "\n",
    "Skewness of the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Skewness of the columns:  \\n\", data.skew(axis='index'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns sby_need and dafted are highly skewed!\n",
    "\n",
    "## <a id='toc6_8_'></a>[Stationarity](#toc0_)\n",
    "\n",
    "For some time series forecasting techniques the data must be stationary (Atwan 2022, p. 308)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using statsmodel for detecting stationarity\n",
    "from statsmodels.tsa.stattools import adfuller, kpss\n",
    "\n",
    "# Iterate over all columns which could be used as model input (so all except n_sby) except column one (date)\n",
    "for column in data.iloc[:,1:].columns.drop(\"n_sby\"):\n",
    "    print(\"Data Strationarity of column\", column)\n",
    "    # Augmented Dickey-Fuller Test\n",
    "    adf_output = adfuller(data[column])\n",
    "    # Decision based on pval\n",
    "    adf_pval = adf_output[1]\n",
    "    if adf_pval < 0.05: \n",
    "        print(\"ADF Decision: Data is stationary\")\n",
    "    else:\n",
    "        print(\"ADF Decision: Data is non-stationary\")\n",
    "    # Kwiatkowski-Phillips-Schmidt-Shin Test\n",
    "    data[column].dropna()\n",
    "    kpss_output = kpss(data[column])\n",
    "    # Decision based on pval\n",
    "    kpss_pval = kpss_output[1]\n",
    "    if kpss_pval >= 0.05: \n",
    "        print(\"KPSS Decision: Data is stationary\")\n",
    "    else:\n",
    "        print(\"KPSS Decision: Data is non-stationary\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some columns which contain non-stationary data and must be considered during the data preparation phase.\n",
    "\n",
    "# <a id='toc7_'></a>[Data Quality](#toc0_)\n",
    "\n",
    "Basically, it is multivariate time series data with good quality. Only the date column was converted for better processing and no missing values could be detected. In some cases, variables show strong correlations with each other what needs to be considered in the data preparation phase and the feature selection phase.\n",
    "The data types of the variables match the specification and the value ranges are mostly plausible, but should be checked for outliers in individual cases. Data of some columns are highly skewed, which must be considered when detecting outliers.\n",
    "In principle, a seasonal pattern and a slight upward trend can be discerned in the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dutySchedule_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6913ccc9ce5f215f64980cc9576bfffc64b3ea55643bf3a7938d14e6344a8d8b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
